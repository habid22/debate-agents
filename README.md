# discourse â€” AI Debate Arena

A sophisticated multi-agent debate system where AI agents with distinct personalities engage in structured debates on any topic. Built with local LLMs via Ollama â€” **100% free, runs entirely on your machine**.

![AI Debate Arena](https://img.shields.io/badge/AI-Multi--Agent-blue)
![Python](https://img.shields.io/badge/Python-3.10+-green)
![Next.js](https://img.shields.io/badge/Next.js-14-black)
![Ollama](https://img.shields.io/badge/Ollama-Local%20LLM-orange)

## âœ¨ Features

### ğŸ¯ Structured 5-Phase Debates
- **Opening Statements** â€” Each agent presents their initial perspective
- **Rebuttals** â€” Agents challenge and respond to each other's arguments
- **Cross-Examination** â€” Direct questioning between opponents
- **Closing Statements** â€” Final arguments and summaries
- **Voting & Synthesis** â€” Agents vote for the most compelling argument + moderator synthesis

### ğŸ¤– 12 Unique AI Agents

**Modern Perspectives:**
| Agent | Personality |
|-------|-------------|
| ğŸ˜Š **The Optimist** | Sees possibility where others see problems, champions progress |
| ğŸ¤” **The Skeptic** | Questions everything, demands evidence over enthusiasm |
| âš–ï¸ **The Pragmatist** | Cuts through ideology, focuses on what actually works |
| ğŸ’¡ **The Innovator** | Challenges conventions, explores unconventional solutions |
| ğŸ–ï¸ **The Veteran** | Pattern recognition from experience, institutional memory |
| ğŸ˜ˆ **The Contrarian** | Attacks consensus, stress-tests ideas |

**Philosophers (4 Core Ethical Frameworks + 2 Wildcards):**
| Agent | Framework | Core Question |
|-------|-----------|---------------|
| ğŸ“œ **Kant** | Deontologist | "Can this be a universal moral law?" |
| ğŸ“Š **Mill** | Utilitarian | "Does this maximize overall wellbeing?" |
| ğŸ›ï¸ **Aristotle** | Virtue Ethicist | "What would a person of good character do?" |
| âš–ï¸ **Rawls** | Justice Theorist | "Is this fair behind a veil of ignorance?" |
| â“ **Socrates** | Dialectician | Probes assumptions through questioning |
| âš¡ **Nietzsche** | Existentialist | Challenges all moral frameworks |

### ğŸ¨ Interactive UI Features
- **Real-time streaming** â€” Watch arguments appear as they're generated
- **Vote & Pin** â€” Upvote compelling points, pin key insights
- **Follow-up Questions** â€” Ask any agent clarifying questions mid-debate
- **Agent Responses** â€” Request one agent to respond to another
- **Save & Export** â€” Save debates locally, copy as markdown
- **Keyboard shortcuts** â€” `Enter` to start, `Esc` to stop

### ğŸ’° 100% Free & Private
- Runs entirely on your machine using Ollama
- No API keys, no subscriptions, no data leaves your computer

## ğŸš€ Quick Start

### Prerequisites

1. **Python 3.10+** â€” [Download](https://python.org)
2. **Node.js 18+** â€” [Download](https://nodejs.org)
3. **Ollama** â€” [Download](https://ollama.ai)

### Installation

#### Step 1: Install Ollama and download a model

```bash
# After installing Ollama from https://ollama.ai, run:
ollama pull mistral:7b
```

This downloads the Mistral 7B model (~4GB). Only needed once.

> **Tip:** For faster responses on lower-end hardware, use `ollama pull llama3.2:3b`

#### Step 2: Set up the Backend

```bash
# Navigate to backend folder
cd backend

# Create virtual environment (recommended)
python -m venv venv

# Activate it
# Windows:
venv\Scripts\activate
# Mac/Linux:
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

#### Step 3: Set up the Frontend

```bash
# Navigate to frontend folder
cd frontend

# Install dependencies
npm install
```

### Running the Application

You need **two terminal windows**:

#### Terminal 1: Start the Backend

```bash
cd backend
# Activate venv if not already active
venv\Scripts\activate  # Windows
# source venv/bin/activate  # Mac/Linux

uvicorn server:app --reload --port 8000
```

You should see: `Uvicorn running on http://127.0.0.1:8000`

#### Terminal 2: Start the Frontend

```bash
cd frontend
npm run dev
```

You should see: `Local: http://localhost:3000`

#### Step 4: Open in Browser

Go to **http://localhost:3000** and start debating! ğŸ‰

## ğŸ® Usage

1. **Enter a topic** or click a sample topic
2. **Select 2-4 agents** â€” mix and match modern perspectives and philosophers
3. **Choose rounds** (more rounds = deeper debate)
4. **Click "Start"** and watch the structured debate unfold

### Debate Flow

```
ğŸ“¢ Opening Statements â†’ âš”ï¸ Rebuttals â†’ â“ Cross-Examination â†’ ğŸ¤ Closing â†’ ğŸ—³ï¸ Voting â†’ ğŸ“Š Synthesis
```

### Interactive Controls

| Action | Description |
|--------|-------------|
| â†‘/â†“ | Vote on arguments |
| ğŸ“Œ | Pin important insights |
| ğŸ’¬ | Ask follow-up questions |
| â†© | Request agent-to-agent responses |
| âŠ/âŠŸ | Expand/collapse arguments |

## ğŸ“ Project Structure

```
debate-agents/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ agents.py        # 12 agent definitions + LLM integration
â”‚   â”œâ”€â”€ arena.py         # 5-phase debate orchestration
â”‚   â”œâ”€â”€ server.py        # FastAPI server with SSE streaming
â”‚   â””â”€â”€ requirements.txt # Python dependencies
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ page.tsx     # Main debate interface (React + Framer Motion)
â”‚   â”‚   â”œâ”€â”€ layout.tsx   # App layout
â”‚   â”‚   â””â”€â”€ globals.css  # Tailwind styles
â”‚   â”œâ”€â”€ package.json     # Node dependencies
â”‚   â””â”€â”€ ...config files
â”‚
â””â”€â”€ README.md
```

## ğŸ”§ API Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/` | GET | Health check and API info |
| `/health` | GET | Health status |
| `/templates` | GET | List available agent templates |
| `/debate` | POST | Start a streaming debate (SSE) |
| `/debate/sync` | POST | Start debate (non-streaming) |
| `/followup` | POST | Ask an agent a follow-up question |
| `/respond` | POST | Request agent-to-agent response |

### Example API Request

```bash
curl -X POST http://localhost:8000/debate \
  -H "Content-Type: application/json" \
  -d '{
    "topic": "Should we regulate AI development?",
    "rounds": 2,
    "agent_templates": ["kant", "mill", "nietzsche"]
  }'
```

## ğŸ› ï¸ Configuration

### Using a Different Model

Edit `backend/agents.py` line 25:

```python
self.model = "mistral:7b"  # Change to any Ollama model
```

Available models:
- `mistral:7b` â€” Default, high quality (recommended)
- `llama3.2:3b` â€” Faster, good quality
- `llama3.2:1b` â€” Fastest, lower quality
- `phi3:mini` â€” Very fast, compact
- `mixtral:8x7b` â€” Highest quality, needs more RAM

### Adjusting Creativity

Edit `backend/agents.py` in `generate_response()`:

```python
options={
    'temperature': 0.8,  # 0.0-1.0, higher = more creative
    'top_p': 0.9,
}
```

## ğŸ› Troubleshooting

### "Connection refused" error
- Make sure the backend is running on port 8000
- Check if Ollama is installed and running (`ollama serve`)

### "Model not found" error
```bash
ollama pull mistral:7b
```

### Slow responses
- Try a smaller model: `ollama pull llama3.2:3b`
- Close other applications to free up RAM
- Ensure Ollama is using GPU if available

### Backend won't start
- Make sure you're in the `backend` folder
- Activate the virtual environment
- Run `pip install -r requirements.txt`

## ğŸš€ Deployment

### Backend (Railway/Render)

1. Push to GitHub
2. Connect to Railway/Render
3. Set build command: `pip install -r requirements.txt`
4. Set start command: `uvicorn server:app --host 0.0.0.0 --port $PORT`

**Note**: For cloud deployment, you'll need to use a cloud LLM API instead of Ollama.

### Frontend (Vercel)

1. Push to GitHub
2. Import to Vercel
3. Set `NEXT_PUBLIC_API_URL` environment variable to your backend URL

## ğŸ§  How It Works

1. **Agent System** â€” Each agent has a unique personality prompt that shapes their reasoning style
2. **Debate Orchestration** â€” The arena manages turn-taking, context passing, and phase transitions
3. **Streaming Responses** â€” Server-Sent Events (SSE) deliver arguments in real-time
4. **Local LLM** â€” Ollama runs the model locally, ensuring privacy and zero cost

## ğŸ“ License

MIT License â€” feel free to use this for your portfolio, learning, or commercial projects.

## ğŸ™ Acknowledgments

- [Ollama](https://ollama.ai) â€” Local LLM runtime
- [FastAPI](https://fastapi.tiangolo.com) â€” Python API framework
- [Next.js](https://nextjs.org) â€” React framework
- [Tailwind CSS](https://tailwindcss.com) â€” Utility-first CSS
- [Framer Motion](https://framer.com/motion) â€” Animation library

---

Built with ğŸ’œ for exploring multi-agent AI systems
