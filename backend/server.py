"""
FastAPI Server - REST API for the debate system with real-time streaming
"""
import json
from typing import Optional
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from pydantic import BaseModel, Field

from agents import Agent, AGENT_TEMPLATES, create_custom_agent
from arena import DebateArena, create_default_debate


# Initialize FastAPI app
app = FastAPI(
    title="AI Debate Arena",
    description="Multi-agent debate system with real-time streaming",
    version="1.0.0"
)

# Enable CORS for frontend
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # In production, specify your frontend URL
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# Request/Response Models
class AgentConfig(BaseModel):
    """Configuration for a custom agent."""
    name: str = Field(..., description="Agent's display name")
    role: str = Field(..., description="Agent's role/title")
    personality: str = Field(..., description="Agent's personality description")
    stance: str = Field(default="neutral", description="Agent's stance: pro, con, or neutral")


class FollowUpRequest(BaseModel):
    """Request for a follow-up question to an agent."""
    topic: str = Field(..., description="The debate topic")
    agent_template: str = Field(..., description="The agent template to ask")
    question: str = Field(..., description="The follow-up question")
    context: list[dict] = Field(default=[], description="Previous debate context")


class AgentResponseRequest(BaseModel):
    """Request for one agent to respond to another."""
    topic: str = Field(..., description="The debate topic")
    responder_template: str = Field(..., description="The agent who will respond")
    target_agent: str = Field(..., description="The agent being responded to")
    target_message: str = Field(..., description="The message being responded to")
    context: list[dict] = Field(default=[], description="Previous debate context")


class DebateRequest(BaseModel):
    """Request to start a debate."""
    topic: str = Field(..., description="The debate topic")
    rounds: int = Field(default=2, ge=1, le=5, description="Number of debate rounds (1-5)")
    agent_templates: Optional[list[str]] = Field(
        default=None,
        description="List of agent template names to use"
    )
    custom_agents: Optional[list[AgentConfig]] = Field(
        default=None,
        description="List of custom agent configurations"
    )


class DebateHistoryResponse(BaseModel):
    """Response containing debate history."""
    topic: str
    rounds: int
    history: list[dict]


# API Endpoints
@app.get("/")
async def root():
    """Health check and API info."""
    return {
        "name": "AI Debate Arena",
        "status": "running",
        "version": "1.0.0",
        "endpoints": {
            "POST /debate": "Start a new debate (streaming)",
            "GET /templates": "List available agent templates",
            "GET /health": "Health check"
        }
    }


@app.get("/health")
async def health_check():
    """Health check endpoint."""
    return {"status": "healthy"}


@app.get("/templates")
async def get_agent_templates():
    """Get list of available agent templates."""
    templates = {}
    for name, agent in AGENT_TEMPLATES.items():
        templates[name] = {
            "name": agent.name,
            "role": agent.role,
            "personality": agent.personality,
            "stance": agent.stance
        }
    return {"templates": templates}


@app.post("/debate")
async def start_debate(request: DebateRequest):
    """
    Start a new debate with streaming responses.
    
    The response is a Server-Sent Events (SSE) stream that yields
    each argument as it's generated by the agents.
    """
    
    def generate_stream():
        """Generator that yields debate events as SSE."""
        
        # Create the arena
        arena = DebateArena(topic=request.topic, rounds=request.rounds)
        
        # Add agents based on request
        if request.custom_agents:
            # Use custom agents
            for config in request.custom_agents:
                agent = create_custom_agent(
                    name=config.name,
                    role=config.role,
                    personality=config.personality,
                    stance=config.stance
                )
                arena.add_agent(agent)
        elif request.agent_templates:
            # Use specified templates
            from agents import get_template_agent
            for template_name in request.agent_templates:
                try:
                    arena.add_agent(get_template_agent(template_name))
                except ValueError as e:
                    yield f"data: {json.dumps({'type': 'error', 'message': str(e)})}\n\n"
                    return
        else:
            # Use default agents
            from agents import get_template_agent
            arena.add_agent(get_template_agent("optimist"))
            arena.add_agent(get_template_agent("skeptic"))
            arena.add_agent(get_template_agent("pragmatist"))
        
        # Run the debate and stream results
        for entry in arena.run_debate():
            yield f"data: {json.dumps(entry)}\n\n"
    
    return StreamingResponse(
        generate_stream(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
        }
    )


@app.post("/debate/sync")
async def start_debate_sync(request: DebateRequest):
    """
    Start a debate and return all results at once (non-streaming).
    Useful for testing or when streaming isn't needed.
    """
    
    # Create the arena
    arena = DebateArena(topic=request.topic, rounds=request.rounds)
    
    # Add agents based on request
    if request.custom_agents:
        for config in request.custom_agents:
            agent = create_custom_agent(
                name=config.name,
                role=config.role,
                personality=config.personality,
                stance=config.stance
            )
            arena.add_agent(agent)
    elif request.agent_templates:
        from agents import get_template_agent
        for template_name in request.agent_templates:
            try:
                arena.add_agent(get_template_agent(template_name))
            except ValueError as e:
                raise HTTPException(status_code=400, detail=str(e))
    else:
        from agents import get_template_agent
        arena.add_agent(get_template_agent("optimist"))
        arena.add_agent(get_template_agent("skeptic"))
        arena.add_agent(get_template_agent("pragmatist"))
    
    # Run the debate and collect all results
    results = list(arena.run_debate())
    
    return {
        "topic": request.topic,
        "rounds": request.rounds,
        "events": results
    }


@app.post("/followup")
async def follow_up_question(request: FollowUpRequest):
    """
    Ask a specific agent a follow-up question.
    Returns streaming response.
    """
    
    def generate_stream():
        from agents import get_template_agent
        
        try:
            agent = get_template_agent(request.agent_template)
        except ValueError as e:
            yield f"data: {json.dumps({'type': 'error', 'message': str(e)})}\n\n"
            return
        
        # Build context string
        context_text = ""
        if request.context:
            context_text = "\n\n".join([
                f"**{c['agent']}** ({c['role']}): {c['message']}"
                for c in request.context
            ])
        
        prompt = f"""You are **{agent.name}**, a {agent.role}.

## Your Personality
{agent.personality}

## Debate Topic
"{request.topic}"

## Previous Discussion
{context_text if context_text else "(No previous discussion)"}

## Follow-up Question
Someone has asked you: "{request.question}"

## Your Task
Answer this follow-up question while staying in character as {agent.name}.
Be specific and provide additional insight or clarification.
Keep your response under 100 words.

## Your Response:"""
        
        try:
            import ollama
            response = ollama.chat(
                model=agent.model,
                messages=[{'role': 'user', 'content': prompt}],
                options={'temperature': 0.8, 'top_p': 0.9}
            )
            message = response['message']['content'].strip()
            
            yield f"data: {json.dumps({'type': 'followup', 'agent': agent.name, 'role': agent.role, 'message': message, 'question': request.question})}\n\n"
        except Exception as e:
            yield f"data: {json.dumps({'type': 'error', 'message': str(e)})}\n\n"
    
    return StreamingResponse(
        generate_stream(),
        media_type="text/event-stream",
        headers={"Cache-Control": "no-cache", "Connection": "keep-alive"}
    )


@app.post("/respond")
async def agent_response(request: AgentResponseRequest):
    """
    Have one agent directly respond to another agent's argument.
    Returns streaming response.
    """
    
    def generate_stream():
        from agents import get_template_agent
        
        try:
            responder = get_template_agent(request.responder_template)
        except ValueError as e:
            yield f"data: {json.dumps({'type': 'error', 'message': str(e)})}\n\n"
            return
        
        # Build context string
        context_text = ""
        if request.context:
            context_text = "\n\n".join([
                f"**{c['agent']}** ({c['role']}): {c['message']}"
                for c in request.context[-5:]  # Last 5 messages for context
            ])
        
        prompt = f"""You are **{responder.name}**, a {responder.role}.

## Your Personality
{responder.personality}

## Debate Topic
"{request.topic}"

## Recent Discussion
{context_text if context_text else "(No previous discussion)"}

## You are responding directly to {request.target_agent} who said:
"{request.target_message}"

## Your Task
Respond directly to {request.target_agent}'s argument above.
Stay in character as {responder.name}.
Be specific - either support, challenge, or add nuance to their point.
Keep your response under 100 words.

## Your Response to {request.target_agent}:"""
        
        try:
            import ollama
            response = ollama.chat(
                model=responder.model,
                messages=[{'role': 'user', 'content': prompt}],
                options={'temperature': 0.8, 'top_p': 0.9}
            )
            message = response['message']['content'].strip()
            
            yield f"data: {json.dumps({'type': 'response', 'agent': responder.name, 'role': responder.role, 'message': message, 'responding_to': request.target_agent})}\n\n"
        except Exception as e:
            yield f"data: {json.dumps({'type': 'error', 'message': str(e)})}\n\n"
    
    return StreamingResponse(
        generate_stream(),
        media_type="text/event-stream",
        headers={"Cache-Control": "no-cache", "Connection": "keep-alive"}
    )


# Run with: uvicorn server:app --reload --port 8000
if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000, reload=True)

